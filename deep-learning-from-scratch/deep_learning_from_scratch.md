## 深度学习入门：基于Python的理论与实现

这里罗列本书学习后，应该掌握的内容

#### 第三章   神经网络

* 感知网络的原型
* 阶跃函数、sigmoid函数区别，深度学习为何使用后者
* 为什么神经网络使用非线性函数
* Relu函数
* 权重符号格式
* 简单三层前向神经网络
* 回归用恒等函数，分类用Softmax函数，为什么
* Softmax函数原理及代码实现，训练时使用，预测一般不使用
* 正则化、批量化的minist简单网络

#### 第四章   神经网络的学习

* 均分误差及其mini batch实现
* one-hot标签
* cross-entropy error及其mini batch实现
* 为什么使用损失函数而不用精度，类似激活函数不选择阶跃函数
* 梯度下降的代码实现

#### 第五章   误差方向传播法

* 误差方向传播算法的数学式和计算图理解
* 链式求导，链式法则在反向传播中的使用
* 加法层和乘法层的前向和反向实现
* 激活函数层的计算图及代码实现
* 多维数组的Affine层，计算图及代码实现
* softmax with loss层实现
* 为什么分类使用交叉熵误差、回归使用平方和误差，反向传播的结果
* 误差反向传播的实现
* 数值微分和解析性数学求解对比及梯度确认

#### 第六章   与学习相关的技巧

* SGD实现及缺点
* momentum实现
* AdaGrad实现
* Adam的原理
* 权重初始值不可以是0或者相同的值，为什么？0无法学习，相同值无法防止权重均一化、瓦解权重对称结构
* 梯度消失原理及解决方法，表现力受限
* sigmoid适合Xavier初始值，为什么
* RELU函数适合He初始值，为什么
* batch norm优点、原理及实现
* 过拟合是什么，产生原因及原理
* 权重衰减L1和L2抑制过拟合原理
* dropout比上面L2优势，实现及原理
* 超参包含哪些？验证方法

#### 第七章   卷积神经网络

* CNN常见结构
* 全链接层的问题及卷积层加入
* 卷积运算、padding、stride概念及计算
* 多维卷积运算的处理流图
* 池化max average，池化层特征
* 卷积层和池化层的实现
* CNN实现
* CNN的可视化
* 各种CNN的发展变化

#### 第八章   深度学习

* 数据增强的方法
* minist最好成绩网络不是很深，简单任务
* 加深网络层的必要性
* alexnet vgg googlenet resnet各自特点
* 神经网络各层占用时间
* R-CNN Selective Search，Fast RCNN，FCN，NIC（CNN+RNN），RNN，多模态处理
* 图像生成GAN，DCGAN
* 无人驾驶SegNet
* 强化学习DQN