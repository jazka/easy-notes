## 深度学习入门：基于Python的理论与实现

这里罗列本书学习后，应该掌握的内容

#### 第三章   神经网络

* 感知网络的原型

  线性：与门、与非门、或门；非线性：异或门(多层感知机)；感知机参数：权重和偏置

* 阶跃函数、sigmoid函数的代码实现，两者区别，深度学习为何使用后者

  sigmoid函数平滑，在极大和极小值附近梯度接近0，但是阶跃函数的梯度除0外，都为0，神经网络学习无法进行

* 为什么神经网络使用非线性函数

  线性叠加，加深层无意义

* Relu函数

  h(x) = max(0, x)，比sigmoid的优势

* 权重符号格式

  右上角：表示第几层的权重、右下角：前面表示当前层的第几个神经元，后面表示前一层的第几个神经元

* 简单三层前向神经网络

  init_network、forward

* 回归用恒等函数，分类用Softmax函数，为什么?

  回归输出连续值，分类输出是离散值，同时考虑反向传播的局部梯度函数

* Softmax函数原理及代码实现，训练时使用，预测一般不使用

  防止溢出，每个值减去输入值的最大值；输出在0.0到1.0之间的实数，sum=1，相当于概率；利用了指数函数exp的单调递增性

* 正则化、批处理的minist简单网络

  像素值从0-255正则化到0.0-1.0；以0为中心分布；数据白化；批处理可以高速运算；get_data、init_network、predict、accuracy

#### 第四章   神经网络的学习

* 均分误差及其mini batch实现

* one-hot标签

  非one-hot标签类型时，y[np.arange(batch_size), t]

* cross-entropy error及其mini batch实现

  维度为1时，扩展维度

* 为什么使用损失函数而不用精度，类似激活函数不选择阶跃函数

  参数变化，精度可能不变或者不连续，梯度为0

* 数值微分实现及作用

  梯度校验

* 梯度下降的代码实现

  微分实现、学习率、迭代步数、更新参数

* mini batch的两层网络训练

  mini batch读取数据、predict、loss、accuracy、numerical_gradient、梯度更新、loss绘图

#### 第五章   误差反向传播法

* 误差反向传播算法的数学式和计算图理解

  数学式通过严格的数据证明，计算图将前向计算转换为变量+操作符的组合，拆解后逐级传递。比如“支付金额
  关于苹果的价格的导数”的值是2.2。这意味着，如果苹果的价格上涨1日元，最终的支付金额会增加2.2日元

* 链式求导，链式法则在反向传播中的使用

  输出关于输入的函数是多级的、复杂的，利用链式求导法则进行逐级计算

* 加法层和乘法层的前向和反向实现

  加法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号；乘法的反向传播会乘以输入信号的翻转值，所以要保存正向传播的输入信号

* 激活函数层的计算图及代码实现

  ReLU：如果正向传播时的输入值小于等于0，则反向传播的值为0，否则为dout * 1

  Sigmoid：y = 1 / (1+exp(-x))，dx = dout * (y * (1-y))

* 多维数组的Affine层，计算图及代码实现

  计算图： X:(N, 2)   W:(2, 3)    b:(3,)   Y:(N, 3)

  out = np.dot(X, W) + b

  db = np.sum(dout, axis=0)

  dW = np.dot(X.T, dout)

  dX = np.dot(dout, W.T)

* softmax with loss层实现

  详细的计算图比较复杂，可以记住简易版，反向传播结果为:（y1 −t1, y2 − t2, y3 − t3），即输出与标签差分

* 为什么分类使用交叉熵误差、回归使用平方和误差，反向传播的结果

  对应Softmax和恒等函数，都是为了获得到（y1 −t1, y2 − t2, y3 − t3）这种简单直接的误差

* 误差反向传播的实现

  loss = cross_entropy_error(y, t)

  dx = (y - t) / batch_size

* 数值微分和解析性数学求解对比及梯度确认

  数值微分简单，但是计算耗时，解析性数学求解高校利用误差反向传播法计算梯度高效，但参数量大、实现复杂，可以使用数值微分进行梯度一致性比较，即梯度确认

#### 第六章   与学习相关的技巧

* SGD实现及缺点
* momentum实现
* AdaGrad实现
* Adam的原理
* 权重初始值不可以是0或者相同的值，为什么？0无法学习，相同值无法防止权重均一化、瓦解权重对称结构
* 梯度消失原理及解决方法，表现力受限
* sigmoid适合Xavier初始值，为什么
* RELU函数适合He初始值，为什么
* batch norm优点、原理及实现
* 过拟合是什么，产生原因及原理
* 权重衰减L1和L2抑制过拟合原理
* dropout比上面L2优势，实现及原理
* 超参包含哪些？验证方法

#### 第七章   卷积神经网络

* CNN常见结构
* 全链接层的问题及卷积层加入
* 卷积运算、padding、stride概念及计算
* 多维卷积运算的处理流图
* 池化max average，池化层特征
* 卷积层和池化层的实现
* CNN实现
* CNN的可视化
* 各种CNN的发展变化

#### 第八章   深度学习

* 数据增强的方法
* minist最好成绩网络不是很深，简单任务
* 加深网络层的必要性
* alexnet vgg googlenet resnet各自特点
* 神经网络各层占用时间
* R-CNN Selective Search，Fast RCNN，FCN，NIC（CNN+RNN），RNN，多模态处理
* 图像生成GAN，DCGAN
* 无人驾驶SegNet
* 强化学习DQN