## 深度学习入门：基于Python的理论与实现

这里罗列本书学习后，应该掌握的内容

#### 第三章   神经网络

* 感知网络的原型

  线性：与门、与非门、或门；非线性：异或门(多层感知机)；感知机参数：权重和偏置

* 阶跃函数、sigmoid函数的代码实现，两者区别，深度学习为何使用后者

  sigmoid函数平滑，在极大和极小值附近梯度接近0，但是阶跃函数的梯度除0外，都为0，神经网络学习无法进行

* 为什么神经网络使用非线性函数

  线性叠加，加深层无意义

* Relu函数

  h(x) = max(0, x)，比sigmoid的优势

* 权重符号格式

  右上角：表示第几层的权重、右下角：前面表示当前层的第几个神经元，后面表示前一层的第几个神经元

* 简单三层前向神经网络

  init_network、forward

* 回归用恒等函数，分类用Softmax函数，为什么?

  回归输出连续值，分类输出是离散值，同时考虑反向传播的局部梯度函数

* Softmax函数原理及代码实现，训练时使用，预测一般不使用

  防止溢出，每个值减去输入值的最大值；输出在0.0到1.0之间的实数，sum=1，相当于概率；利用了指数函数exp的单调递增性

* 正则化、批处理的minist简单网络

  像素值从0~255正则化到0.0~1.0；以0为中心分布；数据白化；批处理可以高速运算；get_data、init_network、predict、accuracy

#### 第四章   神经网络的学习

* 均分误差及其mini batch实现
* one-hot标签
* cross-entropy error及其mini batch实现
* 为什么使用损失函数而不用精度，类似激活函数不选择阶跃函数
* 梯度下降的代码实现

#### 第五章   误差方向传播法

* 误差方向传播算法的数学式和计算图理解
* 链式求导，链式法则在反向传播中的使用
* 加法层和乘法层的前向和反向实现
* 激活函数层的计算图及代码实现
* 多维数组的Affine层，计算图及代码实现
* softmax with loss层实现
* 为什么分类使用交叉熵误差、回归使用平方和误差，反向传播的结果
* 误差反向传播的实现
* 数值微分和解析性数学求解对比及梯度确认

#### 第六章   与学习相关的技巧

* SGD实现及缺点
* momentum实现
* AdaGrad实现
* Adam的原理
* 权重初始值不可以是0或者相同的值，为什么？0无法学习，相同值无法防止权重均一化、瓦解权重对称结构
* 梯度消失原理及解决方法，表现力受限
* sigmoid适合Xavier初始值，为什么
* RELU函数适合He初始值，为什么
* batch norm优点、原理及实现
* 过拟合是什么，产生原因及原理
* 权重衰减L1和L2抑制过拟合原理
* dropout比上面L2优势，实现及原理
* 超参包含哪些？验证方法

#### 第七章   卷积神经网络

* CNN常见结构
* 全链接层的问题及卷积层加入
* 卷积运算、padding、stride概念及计算
* 多维卷积运算的处理流图
* 池化max average，池化层特征
* 卷积层和池化层的实现
* CNN实现
* CNN的可视化
* 各种CNN的发展变化

#### 第八章   深度学习

* 数据增强的方法
* minist最好成绩网络不是很深，简单任务
* 加深网络层的必要性
* alexnet vgg googlenet resnet各自特点
* 神经网络各层占用时间
* R-CNN Selective Search，Fast RCNN，FCN，NIC（CNN+RNN），RNN，多模态处理
* 图像生成GAN，DCGAN
* 无人驾驶SegNet
* 强化学习DQN